# AlexNet总结

## 1、背景

两个重要条件：

​			1、LabelMe,ImageNet等标注软件和数据集的出现

​			2、硬件计算能力的提升（使用GPU作为计算硬件）

## 2、成果

<img src="\img\tmp3B0F.png" alt="tmp3B0F" style="zoom:75%;" />

top-1:表示预测类和标签一致（预测正确）并且概率最高的情况

top-5：表示真实标签在预测的概率排名前5的中。（预测正确的类别概率排名前5）

1CNN : 表示训练一次AlexNet取得验证集错误率40.7%（top-1）

5CNN : 表示训练五次AlexNet取得验证集平均错误率38.1%（top-1）

CNN*是在最后一个池化层之后，额外添加第6个卷积层并使用ImageNet2011（秋）进行预训练然后用ILSVRC-2012微调的准确率

7CNNs* 两个网络进行预训练微调，与5CNNs取均值的准确率

LSVRC-2012上top-5错误率分别为15.3%，取得冠军

## 3、总结

1、使用relu激活函数，dropout层（至今效果很好）

​	 relu优点：①可以使网络训练更快  《--- 求导方便

​						②增加网络的非线性		《---relu为非线性函数

​						③防止梯度消失                《----非饱和性函数导数不会很大也不会过小

​						④使网络更具有稀疏性（泛化性）《----负方向可以使一些神经元输出为0

​	dropout：作用：随机将一定比例的神经元置为0，使输出不依赖某几个特定的神经元

2、softmax和cross entropy

​	softmax作用：将神经元的输出的1000维向量（1000分类）变为按概率分布，概率分布有一下特点：

​							①向量所有值相加和为1

​							②负数变为正数

​							③所有值都在[0,1]之间

  cross entropy（交叉熵）：	常见为交叉熵损失函数，cross entropy 是用来衡量两个概率分布之间的距离的；主要用于计算loss给反向传播提供模型好坏的衡量标准。特点：①值越小，两个概率越接近

​				

3、重叠池化，现在算力充足一般不会使用

4、多卡训练，演变成现在的分布式训练

5、数据增强，提供模型泛化能力

