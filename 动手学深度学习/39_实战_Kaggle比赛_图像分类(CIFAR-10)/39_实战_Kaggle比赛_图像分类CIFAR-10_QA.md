# 39_实战 Kaggle比赛_图像分类(CIFAR-10)QA

## 问题1:老师，深度学习的损失函数一般是非凸的吗

答：损失函数是凸的，但是没有用，整个神经网络是非凸的；整个网络加上损失函数就不是凸函数了，凸函数表示能力有限。

## 问题2:老师，我在训练树叶分类时发现训练集的交叉熵oss大于验证集，但是训练集的acc也是大于验证集的，这是什么问题引起的呢

答：因为加了数据增广，整个是很正常的现象。

## 问题3: normalize参数怎么来的啊？

答：ImageNet数据集上来的，3个通道的均值和方差

## 问题4: train valid ds起啥作用？为什么不把 train dsi和 valid ds放一起组成train valid ds?

答：语义上是加在一起，这样写是为了train函数简单些

## 问题5: drop last= false的情况，如果不够 batch size,补0吗红怎么处理？

答：一般dropLast不补零。①读到最后不够就丢掉，②有多少返回多少③从新采样一部分数据。

## 问题6: weight decay和 lr decay的作用有什么区别吗

答：不一样，weight decay是说更新权重时，权重×0.9999再加上更新值。lr decay是作用在学习率上。Weight = weight*decay- lr *lr_decay* * grad;他们作用是不一样的。lr是用来收敛的；weight_decay 是regnization，使模型数值不要太大； weight decay是统计模型上的参数，lr decay是优化算法上的参数。统计模型决定模型厉害程度，拟合函数的能力程度。lr_decay是优化算法，是一对统计模型，给定损失求解的方法。因为统计模型是非凸的。你没有最优解，只能通过算法去逼近一个值，这个是一个求解的方法。

## 问题7:老师能讲一下 momentum吗

答：给下降方向一个冲量，使函数平滑。

## 问题8:什么样的 scheduler是最好的，最优的，怎么选择

答：LR_step方法很好，但是有点缺陷。推荐使用退火余弦衰减算法。

## 问题10:在完整数据集跑一次的时候，参数还更新么？如果不更新，这一步是不是可以省略

答：这一步主要是使用前一步训练的超参数来重新训练所有的数据。

## 问题11:pred.cpu（） numpy（）会造成gpu和cpu之间数据的搬运，影响效率吗？有什么好的方法能避免？

答：推理部分可以忽略不计，开销很小

## 问题14:老师问个题外的问题，我最近在作目标检测。只要识别一个物体就可以，但是在制作数据集的时候，多标点其他类是不是能让模型泛化能力更强

答：可能多采点样本，效果更好点

## 问题16: Ir decay是不是造成了 weight不要更新太多，能引起类似于 weightdecaye的效果吗？

答：答是有类似效果，SGD做了很强的reglization