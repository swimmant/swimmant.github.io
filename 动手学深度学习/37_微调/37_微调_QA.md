# 37微调_QA

## 问题21:微调这部分是意昧着，神经网络进行不同的目标检测，前面层的网络进行特征提取是通用的么？

答：对的，越底层的层，会越通用，因为抽取底层纹理信息，让神经网络去理解数据形成的模版，我们认为是通用的

## 问题22:老师，数据不平衡问题对特征提取器影响大还是分类器影响大？

答：数据不平衡，对特征提取影响不是很大，对越上面的层会影响比较大，数据不平衡通常是标号不平衡，标号不平衡主要是对上次的影响。

## 问题23:假设有A,B两个数据集，都很大，A是 Imagenet,B是医学图片。如果我要识别癌症，那是用 retrained A的现成的模型然后再加上B进行finetuning的效果好，还是直接用B从头进行 train比较好？

答：imageNet主要是生活常见的图片，两个数据集差别太大，可能重头训练效果好一些，可以找一些医学生的pretrain的model，不要找imageNet上的预训练模型

## 问题24: fine tuning是不是就是 transfer training?两者有不同吗？

答：就是啦

## 问题25:老师，重用标号的话，对于无关的标号是直接删除吗，原始模型中没有的标号怎么加进去呢

答：对的，书中有练习，将对应标号的权重拎出来，别的不要；原始数据没有的标号就随机。

## 问题26:标号那块，是不是都是 labele的名称字符串，对应到数字上？微调中有相同标号怎么做原始标号和目标标号的对应

答：可能需要坐下语义匹配

## 问题27:老师，微调就是 transformer learning吗？

答;transfer  learning，微调是迁移学习中的一种算法。

## 问题28: Imgnet是比较简单的话，能从哪些地方获取一些更好的可用于微调的模型

答：微调的模型也是公司的财产，这个需要自己找一下。

## 问题29:如果源数据集和囯标数据集差异很大，微调的效果会下降吗，例如、imagenet上的模型用到医疗影像分类

答，会，源数据集最后比目标数据集更大，最好更相似。

## 问题30:老师，微调的话，源数据集中的样本是否必须包含目标数据集里的类别？

答：不需要完全一样

## 问题31:为什么微调中的归ー化保持一致很重要？是为了保留数据分布信息吗？

答：Normalization可以认为是网络中的一块，Normalization可以换成batchNormalization的那个Layer；数据增强中ToTensor后面的属于网络架构的那一块。在resnet18在copy时没有将Normalization处理Copy过去。这是我们需要手动处理一下。如果resnet18中有normalization的那一层的话，我们在数据增强时是不需要Normalization的。

## 问题32: normalize.里的那些参数(2行3列)从哪里来的？

答：是从ImageNet上图片的3个通道的均值和方差。

## 问题33: Finetune需要更改 normalization的参数为自己的数据集的均值和方差吗？

答：现在可能不需要手动加入了，只是加入BatchNormlization后不需要再设置了

## 问题34:auto- gluon i这些自动化框架会加入微调吗？

答：会做微调，微调不会让你变差。

## 问题35:老师，请向比常用的CV预训练模型有哪些？现在流行的Transformer CV预训练模型有没有？

答：resNet系列

## 问题36:关于“重用分类器权重”，对于ー个80类的数据集，只想选用其中5类，加上另外的4类，怎么重用这5类的权重呢？有什仏快速的方式么？我能想到的就是手动提取

答:现在还没有比较好的方法。手动去匹配下你的target数据集和source数据集的有哪些label，写python脚本，对比下。这个用的不多

## 问题37:老师好，训练集的精度为什么一开始很高，然后急剧下降后再稳定呢？

答：这个是随机性的问题，

## 问题38:老师，微调是直接把别人在 imagenet上训练好的模型参数拿来当作自己模型的初始化吗？还是每次自己先用模型在 imagenet?数据集上跑一边,把参数记下来再跑自己的模型呢？

答：自己有计算资源可以自己Pretrain。也可以使用别人的

## 问题39:计算损失的时候不都是用labe对应的标号么，这块用微调对应的话感觉不太知道怎么操作

答：建议看下练习题，有段代码关于这个的

## 问题40:老师，请问微调在学习率上还有什么有用的技巧吗

答：微调对学习率不敏感

## 问题41:老师，迁移学习國定源模型中的层与目标模型的对接的层之间有影向吗？

答：这是一种正则化，需要自己调

## 问题43:是不是已经有过随机是选择层的的实验了呢？如何选择迁移那些层效果更好呢，就是靠测试

答:有的，可以做循环遍历下，固定i层就行了。很贵