# 卷积层QA

## 问题17:100个神经元的单mlp,跟100维的全连接层，区别是什么？

> **答：没有什么区别，100个神经元的单mlp会有一个100维的全连接层**

## 问题18:为什么要权重变形？为什么要重新索引？

> **答：本次课主要是说怎么样对全连接层做变化做限制得到卷积层；**

## 问题19:最开始的变换是怎么做的？图像从挨个的线性像素集变成二维集？

> **答：图片读进来就是二维的，做softmax时reshape成一维**

## 问题21:为什么不应该看那么远？感受野不是越大越好吗

> **答：这个和全连接层的层大但浅没有层小但深的MLP效果好。**

## 问题22:老师，二维卷积层，有没有可能同时使用两个不同尺寸的 Kernel进行计算，然后再计算出一个更合适的 Kernel,从而提高特征提取的性能。

> **答：弹幕好有趣：低情商：论文看的还不够呀！高情商：10年前的话，google net那篇论文就是你的啦**

## 问题23:怎么理解卷积是反过来走的？卷积公式里面有负号的原因？

> **答：卷积是从信号处理过来的，傅里叶变换出来就是反的**

## 问题26:请问卷积的数学含义是什么？

> **答：可以查下：数字信号处理**

## 问题27:老师，在做房价竟赛时，自己构建的模型，画出来的损失随迭代次数变化图抖动的特别厉害，而不是像老师书上的例子，随着迭代次数增加损失很平滑的越来越小，这是什么原因呢？

> **答：两种可能①数据变化太大，抖动但在下降是没关系的。可以把批量大小给大点**