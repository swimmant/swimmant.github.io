# 1ã€å±‚å’Œå—


```python
import torch 
from torch import nn
from torch.nn import functional as F      #å®šä¹‰äº†ä¸€äº›æœ‰ç”¨çš„å‡½æ•°

net = nn.Sequential(nn.Linear(20,256),
                    nn.ReLU(),
                    nn.Linear(256,10)
                   )
X = torch.rand(2 ,20)         #2ä¸ºæ‰¹å¤„ç†å¤§å°
net(X)
```




    tensor([[ 0.0575,  0.0646, -0.1531, -0.2502,  0.2134, -0.2487,  0.1119, -0.2793,
             -0.0771,  0.2374],
            [ 0.0821, -0.1458, -0.1899, -0.3212,  0.2784, -0.1832,  0.1434, -0.3080,
             -0.2397,  0.1752]], grad_fn=<AddmmBackward>)



nn.Sequential å®šä¹‰ä¸€ç§ç‰¹æ®Šçš„Module

## 1.1ã€è‡ªå®šä¹‰MLP


```python

class MLP(nn.Module):                               #ç»§æ‰¿nn.Module
    def __init__(self):                             #å®šä¹‰ç»„ä»¶
        super().__init__()
        self.hidden = nn.Linear(20,256)             #å®šä¹‰è¾“å…¥ä¸º20è¾“å‡ºä¸º256çš„çº¿æ€§å±‚
        self.out = nn.Linear(256,10)
    def forward(self ,X):                           #å®šä¹‰å‰å‘ä¼ æ’­
        return self.out(F.relu(self.hidden(X)))
        
```


```python
net = MLP()
net(X)
```




    tensor([[ 0.1075,  0.1499,  0.0019, -0.1983,  0.1055,  0.0090,  0.0848,  0.0717,
              0.2854, -0.1110],
            [ 0.1165,  0.0994, -0.0794, -0.2184,  0.1116, -0.0013,  0.0785, -0.0218,
              0.3297, -0.1320]], grad_fn=<AddmmBackward>)



## 1.2ã€é¡ºåºå—


```python
class MySequentail(nn.Module):
    def __init__(self,*args):
        super().__init__()
        for block in args:                            #
            self._modules[block] = block               # ç‰¹æ®Šçš„å®¹å™¨ï¼ŒæŒ‰åºçš„å­—å…¸
    def forward(self,X):
        for block in self._modules.values():
            X = block(X)
        return X
net = MySequentail(nn.Linear(20,256),nn.ReLU(),nn.Linear(256,10))
net(X)
```




    tensor([[ 0.0464, -0.1645,  0.0924, -0.0242,  0.1961,  0.2435,  0.0517, -0.0803,
             -0.0844, -0.1829],
            [ 0.1450, -0.2823,  0.0479, -0.0597,  0.1657,  0.1154,  0.0692, -0.0372,
             -0.1532, -0.2183]], grad_fn=<AddmmBackward>)



## 1.3ã€çµæ´»æ„é€ å‡½æ•°ï¼Œåœ¨æ­£å‘ä¼ æ’­ä¸­æ‰§è¡Œä»£ç 


```python
class FixedHiddenMLP(nn.Module):           #åœ¨initå’Œforwardä¸­å¯ä»¥åšå¤§é‡çš„è‡ªå®šä¹‰è®¡ç®—
    def __init__(self):
        super().__init__()
        self.rand_weight = torch.rand((20,20),requires_grad=False)    #éšæœºweightä¸å‚ä¸è®­ç»ƒ
        self.linear = nn.Linear(20,20)
    def forward(self ,X):
        X =  self.linear(X)                           #å…ˆåšå®Œlinear
        X = F.relu(torch.mm(X,self.rand_weight)+1)    #å’Œéšæœºweightsåšä¹˜æ³•
        X = self.linear(X)                            #åœ¨è°ƒç”¨linear
        while X.abs().sum()>1:                       #å¯¹Xç»å¯¹å€¼æ±‚å’Œå¦‚æœå¤§äºä¸€ï¼Œé™¤äºŒ
            X/=2
        return X.sum()
net = FixedHiddenMLP()
net(X)
```




    tensor(0.1398, grad_fn=<SumBackward0>)



## 1.4ã€åµŒå¥—ä½¿ç”¨Sequentialæˆ–è‡ªå®šä¹‰æ–¹æ³•è‡ªç”±ç»„åˆæ­é…


```python
class NestMLP(nn.Module):
    def __init__(self):
        super().__init__()
        self.net = nn.Sequential(nn.Linear(20,64),nn.ReLU(),
                                 nn.Linear(64,32),nn.ReLU()
                                )
        self.linear = nn.Linear(32,16)
        
    def forward(self,X):
        return self.linear(self.net(X))

chimra = nn.Sequential(NestMLP(),nn.Linear(16,20),FixedHiddenMLP())
chimra(X)
```




    tensor(-0.0030, grad_fn=<SumBackward0>)



## 2ã€å‚æ•°ç®¡ç†
    é¦–å…ˆå…³æ³¨å•éšè—å±‚çš„å¤šå±‚æ„ŸçŸ¥æœº


```python
import torch
from torch import nn

net = nn.Sequential(nn.Linear(4,8),nn.ReLU(),nn.Linear(8,1))
X = torch.rand(size=(2,4))
net(X)
```




    tensor([[-0.3017],
            [-0.3544]], grad_fn=<AddmmBackward>)



### 2.1 å‚æ•°è®¿é—®
    å°†æ¯ä¸€å±‚ä¸­çš„æƒé‡æ‹¿å‡ºæ¥

sequentialå¯ä»¥çœ‹åšä¸ºpython çš„list ï¼› net[2]ç›¸å½“äºæ‹¿åˆ°nn.Linear(8,1)


```python
print(net[2].state_dict())
```

    OrderedDict([('weight', tensor([[-0.3533,  0.0262, -0.1229, -0.0872,  0.3234, -0.3202, -0.2343,  0.2017]])), ('bias', tensor([-0.1021]))])


### 2.2ã€è®¿é—®å…·ä½“çš„å‚æ•°


```python
print(type(net[2].bias))
print(net[2].bias)
print(net[2].bias.data)     #è®¿é—®æ•°æ®ï¼Œå…¶ä¸­è¿˜æœ‰æ¢¯åº¦
```

    <class 'torch.nn.parameter.Parameter'>
    Parameter containing:
    tensor([-0.1021], requires_grad=True)
    tensor([-0.1021])



```python
net[2].weight.grad == None
```




    True




```python
### 2.3ã€ä¸€æ¬¡æ€§è®¿é—®æ‰€æœ‰å‚æ•°
print(*[(name,param.shape) for name,param in net[0].named_parameters()])
print(*[(name,param.shape) for name,param in net.named_parameters()])                #* æ˜¯å¯¹listè¿›è¡Œæ‹†è§£ï¼Œè®¿é—®lietä¸­æ¯ä¸€ç»„æ•°æ®ï¼Œ è§£åŒ…
```

    ('weight', torch.Size([8, 4])) ('bias', torch.Size([8]))
    ('0.weight', torch.Size([8, 4])) ('0.bias', torch.Size([8])) ('2.weight', torch.Size([1, 8])) ('2.bias', torch.Size([1]))



```python
net.state_dict()['2.bias'].data          #é€šè¿‡æŸä¸€å±‚çš„åå­—ç›´æ¥è®¿é—®å…¶ä¸­æ•°å€¼   
```




    tensor([-0.1021])



### 2.3ã€åµŒå¥—ç½‘ç»œä¸­æ”¶é›†å‚æ•°


```python
def block1():
    return nn.Sequential(nn.Linear(4,8),nn.ReLU(), nn.Linear(8,4),nn.ReLU())

def block2():
    net = nn.Sequential()
    for i in range(4):
        net.add_module(f'block{i}',block1())
    return net

rgnet = nn.Sequential(block2(),nn.Linear(4,1))
rgnet(X)
```




    tensor([[-0.2302],
            [-0.2303]], grad_fn=<AddmmBackward>)




```python
print(rgnet)
```

    Sequential(
      (0): Sequential(
        (block0): Sequential(
          (0): Linear(in_features=4, out_features=8, bias=True)
          (1): ReLU()
          (2): Linear(in_features=8, out_features=4, bias=True)
          (3): ReLU()
        )
        (block1): Sequential(
          (0): Linear(in_features=4, out_features=8, bias=True)
          (1): ReLU()
          (2): Linear(in_features=8, out_features=4, bias=True)
          (3): ReLU()
        )
        (block2): Sequential(
          (0): Linear(in_features=4, out_features=8, bias=True)
          (1): ReLU()
          (2): Linear(in_features=8, out_features=4, bias=True)
          (3): ReLU()
        )
        (block3): Sequential(
          (0): Linear(in_features=4, out_features=8, bias=True)
          (1): ReLU()
          (2): Linear(in_features=8, out_features=4, bias=True)
          (3): ReLU()
        )
      )
      (1): Linear(in_features=4, out_features=1, bias=True)
    )


## 3ã€å†…ç½®åˆå§‹åŒ–


```python
def init_normal(m):                                  #å®šä¹‰æ­£å¤ªåˆ†å¸ƒ
    if type(m)==nn.Linear:                           #å¦‚æœæ˜¯çº¿æ€§å±‚åšå‡å€¼ä¸º0ï¼Œæ–¹å·®ä¸º0.01çš„åˆå§‹åŒ–
        nn.init.normal_(m.weight,mean=0,std=0.01)    #æœ‰_ çš„å‡½æ•°è¡¨ç¤ºæ˜¯æ›¿æ¢å‡½æ•°ï¼ŒåŸåœ°æ“ä½œ ï¼Œå°†å…¶ä¸­çš„weightsæ›¿æ¢æ‰
        nn.init.zeros_(m.bias)
net.apply(init_normal)
net[0].weight.data[0],net[0].bias.data[0]
```




    (tensor([0.0060, 0.0060, 0.0031, 0.0024]), tensor(0.))




```python
def init_constant(m):
    if type(m) == nn.Linear:
        nn.init.constant_(m.weight,1)
        nn.init.zeros_(m.bias)
net.apply(init_constant)
net[0].weight.data[0],net[0].bias.data[0]
```




    (tensor([1., 1., 1., 1.]), tensor(0.))



### 3.1ã€å¯¹æŸäº›å—ä½¿ç”¨ä¸åŒçš„åˆå§‹åŒ–æ–¹æ³•


```python

def xavier(m):
    if type(m) == nn.Linear:
        nn.init.xavier_uniform_(m.weight)

def init_42(m):
    if type(m) == nn.Linear:
        nn.init.constant_(m.weight,42)

net[0].apply(xavier)                    #å¯¹ç¬¬ä¸€ä¸ªå±‚ä½¿ç”¨Xavieråˆå§‹åŒ–
net[2].apply(init_42)                   #å¯¹æœ€åä¸€ä¸ªå±‚ä½¿ç”¨init_42çš„åˆå§‹åŒ–
print(net[0].weight.data[0])
print(net[2].weight.data)
```

    tensor([ 0.0405, -0.6049, -0.5710,  0.0473])
    tensor([[42., 42., 42., 42., 42., 42., 42., 42.]])


### 3.2 è‡ªå®šä¹‰åˆå§‹åŒ–


```python
def my_init(m):
    if type(m) == nn.Linear:
        print(
            "Init",
            *[(name,param.shape) for name , param in m.named_parameters()][0]
        )
        nn.init.uniform_(m.weight,-10,10)
        m.weight.data *= m.weight.data.abs() >=5         # å½“å‰æƒé‡æ•°æ®ä¹˜ä»¥(è‡ªå·±ç»å¯¹å€¼ä¸­ï¼Œä¿ç•™å¤§äºç­‰äº5çš„å€¼ï¼Œå…¶ä½™ä¸º0ï¼‰
net.apply(my_init)
net[0].weight[:2]
```

    Init weight torch.Size([8, 4])
    Init weight torch.Size([1, 8])





    tensor([[ 8.5630, -0.0000, -0.0000,  0.0000],
            [ 0.0000,  0.0000, -8.5134, -6.0404]], grad_fn=<SliceBackward>)




```python
# æš´åŠ›ç›´æ¥ä¿®æ”¹
net[0].weight.data[:] +=1
net[0].weight.data[0,0] = 42
net[0].weight.data[0]
```




    tensor([42.,  1.,  1.,  1.])



### 3.3ã€å‚æ•°ç»‘å®š
    å°åº”ç”¨ï¼šåœ¨ä¸€äº›å±‚ä¹‹é—´shareä¸€äº›paramï¼Œè¿ä¸ªè¾“å…¥æ•°æ®æµè¿›æ¥å¦‚ä½•å…±äº«æƒé‡
    å…ˆæ„é€ sharedå±‚ï¼Œæ„é€ å‡ºæ¥æ—¶å‚æ•°å°±ç”Ÿæˆ


```python
shared = nn.Linear(8,8)
net = nn.Sequential(nn.Linear(4,8),nn.ReLU(),shared,nn.ReLU(),shared,nn.ReLU(),nn.Linear(8,1))

net(X)
print(net[2].weight.data[0] == net[4].weight.data[0])
net[2].weight.data[0,0] == 100
print(net[2].weight.data[0] == net[4].weight.data[0])
```

    tensor([True, True, True, True, True, True, True, True])
    tensor([True, True, True, True, True, True, True, True])


## 4ã€è‡ªå®šä¹‰å±‚
    æ„é€ ä¸€ä¸ªæ²¡æœ‰ä»»ä½•å‚æ•°çš„è‡ªå®šä¹‰å±‚


```python
import torch 
import torch.nn.functional as F
from torch import nn

class CenteredLayer(nn.Module):
    def __init__(self):
        super().__init__()
    
    def forward(self ,X):
        return X - X.mean()
layer = CenteredLayer()
layer(torch.FloatTensor([1,2,3,4,5]))
```




    tensor([-2., -1.,  0.,  1.,  2.])



### 4.1 å°†è‡ªå·±æ„é€ çš„å±‚æ”¾å…¥sequential


```python
net = nn.Sequential(nn.Linear(8,128),CenteredLayer())

Y = net(torch.rand(4,8))
Y.mean()
```




    tensor(1.8626e-09, grad_fn=<MeanBackward0>)



### 4.2 è‡ªå®šä¹‰å¸¦æœ‰å‚æ•°çš„å±‚


```python
class MyLinear(nn.Module):
    def __init__(self,in_units,units):
        super().__init__()
        self.weight = nn.Parameter(torch.randn(in_units,units))  #ä¸¤å‚æ•°ï¼Œ éšæœºåˆå§‹åŒ–(è¾“å…¥å¤§å°ï¼Œè¾“å‡ºå¤§å°)çŸ©é˜µ
        self.bias = nn.Parameter(torch.randn(units,))               #randn æ­£æ€åˆ†å¸ƒ  ï¼›åŠ é€—å·æ˜¯åˆ›å»ºåˆ—å‘é‡
        
    def forward(self, X):
        linear = torch.matmul(X,self.weight.data) + self.bias.data
        return F.relu(linear)

dense = MyLinear(5,3)
dense.weight
```




    Parameter containing:
    tensor([[ 0.0804, -0.5038, -1.3182],
            [-0.2190,  0.1457,  1.5468],
            [ 0.0219, -0.7152, -0.6811],
            [-0.8235,  2.0442,  1.8378],
            [-1.4888, -0.1597,  0.9653]], requires_grad=True)



###  4.3 è‡ªå®šå±‚ç›´æ¥æ‰§è¡Œæ­£å‘ä¼ æ’­è®¡ç®—


```python
dense(torch.rand(2,5))
```




    tensor([[1.1066, 1.1407, 0.4707],
            [0.1582, 2.8575, 1.7879]])



###  4.4 è‡ªå®šå±‚æ„å»ºæ¨¡å‹


```python
net = nn.Sequential(MyLinear(64,8),MyLinear(8,1))
net(torch.rand(2,64))
```




    tensor([[4.2691],
            [0.0000]])



## 5ã€è¯»å†™æ–‡ä»¶
    åŠ è½½å’Œä¿å­˜å¼ é‡


```python
%ls
```

    04 æ•°æ®æ“ä½œ + æ•°æ®é¢„å¤„ç†ã€åŠ¨æ‰‹å­¦æ·±åº¦å­¦ä¹ v2ã€‘.ipynb  [0m[01;34md2l-zh[0m/
    16 PyTorch ç¥ç»ç½‘ç»œåŸºç¡€ã€åŠ¨æ‰‹å­¦æ·±åº¦å­¦ä¹ v2ã€‘.ipynb   V10å¤šå±‚æ„ŸçŸ¥æœº.ipynb



```python
x = torch.arange(4)
torch.save(x,'x-file')
x2 = torch.load("x-file")
x2
```




    tensor([0, 1, 2, 3])




```python
%ls
```

    04 æ•°æ®æ“ä½œ + æ•°æ®é¢„å¤„ç†ã€åŠ¨æ‰‹å­¦æ·±åº¦å­¦ä¹ v2ã€‘.ipynb  [0m[01;34md2l-zh[0m/              x-file
    16 PyTorch ç¥ç»ç½‘ç»œåŸºç¡€ã€åŠ¨æ‰‹å­¦æ·±åº¦å­¦ä¹ v2ã€‘.ipynb   V10å¤šå±‚æ„ŸçŸ¥æœº.ipynb


### 5.1ã€å­˜ä¸€ä¸ªå¼ é‡åˆ—è¡¨ï¼Œç„¶åå°†å®ƒä»¬è¯»å›å†…å­˜


```python
y = torch.zeros(4)
torch.save([x,y],'x-file')
x2,y2 = torch.load("x-file")
(x2,y2)
```




    (tensor([0, 1, 2, 3]), tensor([0., 0., 0., 0.]))



### 5.2ã€å†™å…¥å¹¶è¯»å–å­—ç¬¦ä¸²æ˜ å°„çš„å¼ é‡å­—å…¸


```python
mydict = {'x':x , 'y':y}
torch.save(mydict,'mydict')
mydict2 = torch.load('mydict')
mydict2
```




    {'x': tensor([0, 1, 2, 3]), 'y': tensor([0., 0., 0., 0.])}



### 5.3ã€åŠ è½½å’Œä¿å­˜æ¨¡å‹å‚æ•°  
    pytorchä¸å­˜å‚¨è®¡ç®—å›¾ï¼Œåªå­˜å‚¨æƒé‡å‚æ•°ï¼ŒtorchScriptå­˜å‚¨è®¡ç®—å›¾


```python


class MLP(nn.Module):
    def __init__(self):
        super().__init__()
        self.hidden = nn.Linear(20,256)
        self.output = nn.Linear(256,10)
    def forward(self,x):
        return self.output(F.relu(self.hidden(x)))
    
net = MLP()
X = torch.randn(size=(2,20))
Y = net(X)
```


```python
# å°†æ¨¡å‹å‚æ•°å­˜å‚¨ä¸€ä¸ªmlp.params
torch.save(net.state_dict(),'mlp.params')
```

å®ä¾‹åŒ–åŸå§‹å¤šå±‚æ„ŸçŸ¥æœºæ¨¡å‹ï¼Œç›´æ¥è¯»å–æ–‡ä»¶ä¸­å­˜å‚¨çš„å‚æ•°


```python
clone = MLP()
clone.load_state_dict(torch.load("mlp.params"))
clone.eval()
```




    MLP(
      (hidden): Linear(in_features=20, out_features=256, bias=True)
      (output): Linear(in_features=256, out_features=10, bias=True)
    )




```python
Y_clone = clone(X)                   #cloneä¸€ä¸ªç½‘ç»œï¼Œè¾“å…¥å’ŒåŒæ ·ä¹‹å‰çš„éšæœºXï¼Œæ‹¿åˆ°Y è¿›è¡Œå¯¹æ¯”
Y_clone == Y
```




    tensor([[True, True, True, True, True, True, True, True, True, True],
            [True, True, True, True, True, True, True, True, True, True]])


