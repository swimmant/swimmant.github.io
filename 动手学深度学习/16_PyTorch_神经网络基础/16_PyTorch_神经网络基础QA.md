# 16 PyTorch 神经网络基础QA

## 问题1:想请教一下，在将类别变量转换成伪变量的时候内存炸掉了怎么办

> **答：竞赛中处理字符特征时使用one-hot时当字符太多可能导致内存爆炸。**
>
> ​		**两个办法：①：使用稀疏矩阵来存储，一些元素就可以忽略掉**
>
> ​							**②：真遇到这种情况，其实不应该使用one-hot方法，该方法太低效。像竞赛中的地址或者个人描述，可以采用beg for words,将其中每个词拿出来。**

## 问题3:MLP层数，以及每一层单元数怎么设置呀？有大致的规律么，比如 Input size-> input size/2-> input size/4,还是主要靠cV寻参？

> **答：主要靠自己搜参，可以看看MLPQA部分。**

## 问题4:实例化后，不用调用实例方法，就可以net(X),是因为父类实现了魔法方法吗？

> **答：net(X)调用`net.__call__()`函数，nn.Module中将forward函数等价于call函数了；**

## 问题6: kaiming初始化是按什么规则初始化的？

> **答：查看详细论文，初始化不会太影响精度**

## 问题7:我们创建好网络之后 torch是按什么规则给参数初始化的？

> **答：早期的版本用Xavier， 后期的版本用kaiming_normal ；目前我所知1.1版本之后是用 kaiming_normal ；本机torch1.8.1 中卷积参数初始化是kaiming_uniform**

## 问题8:自定义的激活函数如果是非可导的话auto-grad是否可以求出导数还是必须先自定义导函数

> **答:其实没有不能导的函数，只是有些函数有断点，数值运算中不存在不可导**