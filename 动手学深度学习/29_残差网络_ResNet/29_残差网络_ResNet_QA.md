# 29_残差网络ResNet_QA

## 问题15:为什么 lenet batch size大于1000收敛会有问题？会有什么样的问题？nan还是？

答：不是收敛有问题，是收敛很慢

## 问题16:f(x)=x+g(x),这样就能保证至少不会变坏吗？如果g(x)不是变好也不是什么都不干，而是变坏了呢？

答：只有当g(x)变好时才能拿到loss获得梯度进行更新。当模型发现g(x)很难训练或者训练没有什么变化，他就拿不到梯度，resnet加深时通常不会是模型变坏

## 问题17:cos学习率会比step或者固定学习率好吗？

答：cos学习率也挺好的，在前面有足够时间做微调，主要是简单

## 问题20:请问在init里为什么定义两个bn,这两个bn的参数一样？为什么定义了 self relu在 forward里面没有用？麻烦老师可以解释解释nn.RelU( inplace=True) inplace这个参数么？？谢谢

答；有两个边，每个边有自己的参数学习。inplace原地更新

## 问题22:训练aCc是不是正常训练时就是会稍微大于测试acc?这是不是意味着永远达不到100%识别？

答：测试精度可能比训练精度高

## 问题1:学习率可不可以让靠近输出的小一些，靠近输入的大一些，这样会不会就可以缓解梯度消失的问题？

答：可以的，但是不知道大多数小多少

## 问题2:为什么深层的网络，底层比较难训练？是因为它拿到的梯度一般比较小？

答：对的，梯度累乘会使梯度越来越小，梯度值和误差值已有一定关系的，前面层会吸收些误差，使得误差越来越小，梯度越来越小。