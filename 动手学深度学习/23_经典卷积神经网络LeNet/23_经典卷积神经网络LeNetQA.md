# 23_经典卷积神经网络LeNet_QA

## 问题9:沐神，池化和卷积是不是更适合图像这类型的数据，而对于时序性数据（做分类）是不是不适用这类数据？

答：时序可以使用卷积，文本分类就是可以使用一维卷积；池化可用可不用

## 问题10: Lenet的第二个卷积层通道数增加到了16,这意味信息被放大了吗？或者说信息在通道中是怎么流通的？

答：高宽减半时通道数翻倍；当空间信息压缩时，增加的通道数使得可以识别的模式变多了，同样一个像素的信息变多了。实际上整体信息还是往下减得一个过程。

## 问题11:为什么用View而不用 reshape呢？

答：view对数据构造不会发生变化，内存中表示什么形状是可以表示的，reshape会做一些数据copy。

## 问题12:对于选择深度学习模型是用mlp还是cnn,是一种试的心态来选择 还是说应该先从理论上推导，如果行得通再选择用哪一个模型？

答：当输入数据比较大时，可以使用mlp；当数据图片是200*200，mlp就没戏了，基本上overfiting；①你得跑得动 ②跑得动的情况下挑个简单的

## 问题13:Conv2d6->16,16个通道是怎么取前面6个通道的信息的（每个都取，还是按一定组合取）？

答：16个通道的每个通道都会对前面的6个通道信息做融合，按组合取加权

## 问题14:输出通道增多，这些增加的信息是些什么？可以理解是随机扩充的图片信息么？

答：输出通道很大程度可以认为是匹配某一种模式；比如像手写数据集，横、竖、勾、纹理等等，每个通道匹配各自的一些颜色，纹理等一些特征。越下层匹配的越底层，越高层识别的越大。

## 问题15:卷积层输出通道数的选择有什么理由吗？比如，刚才讲的输出通道数6和16,改成5和15可以吗？

答：可以试一下；可以，没有过多变化

## 问题16:像这些经典的网络，有没有必要把每层的参数，像channels, strides, padding这些都背下来？（面试可能会问）

答：理解网络如何运行

## 问题17:池化层一般用max还是aVg,用max的话会不会损失很多信息？

答：都会损失信息，用max是只关心最大的信号量，maxpooling可能比avgpooling更好一些

## 问题18: Lenet当时是用什么语言实现的呢？ Lenetp的卷积层是不是没有加bias?

答：最早有c++,后面用LUA

## 问题19:神经网络的准确率一般到多少算是比较好的？

答：取决于数据，关键取决于阈值。让人使用起来不会反感就行。

## 问题20:老师，为什么用了一层卷积，就从1个通道变成了6个通道？是每个通道用了不同的卷积核吗？

答：因为输入是灰度图，所以输入是1；有6个卷积核所以输入是6

## 问题21:想请教一下 autogloun是否能做时序任务，昨天试了下表格预测模块做时序任务，结果不理想，是我没操作对吗？

答：能做，没有仔细做时序任务

## 问题22:能介绍一下Conv2d和Con3d以及 Maxpool2d和3d的差別、以及各自适用的场景吗？

答：医学、视频，卫星地图

## 问题23:课程网站有个练习是将 sigmoid替换为relu,四个 sigmoid全替换以后模型基本不收敛，有什么原因吗？一般用relu应该比 sigmoid更容易收敛？

答：调学习率,0.1，收敛了

## 问题24:图像识别出来的特征(颜色，形状)可以打印出来吗，或者怎么去寻找网络到底学习了什么？

答：cnn Explainer

## 问题25:在跑的动的情况下，中间计算层的输出通道風量调大么？

答：不一定，大了会overfiting

## 问题26:老师，想问一下目前的卷积神经网络或其他深度学习网络是不是都需要较多的训练数据，如果训练数据体量很小是否不适合用深度学习？是否有无监督/或小训练集的深度学习？

答：不需要很多数据集，通过预训练

## 问题27:请问训练后的权重能做 Ivisualizaiton进行分析么？

答：工作很多，但是好看但是没用

## 问题28:卷积网络等网络是把非结构化数据转換成结构化数据然后来解決问题，这个想法可以通用在深度学习领域吗？

答：图片，语言，文本，通过结构化模型抽取语义等信息；这可以作图，树等结构化数据。神经网络是一种语言，不管数据是结构化还是非结构化，神经网络把其中的语义信息抽取出来