# 28_批量归一化QA

## 问题1:请问，老师还像之前在讲 xavier的时候，也讲过类似的 normalization的东西，这个和这里的BN有什么区别，麻烦讲一下，谢谢

答：本质上没什么区别，目的是为了比较稳定，使用Xavier是说在模型初始化时选择值使得模型相对稳定，归一化是在模型训练过程中使得模型相对比较稳定。 他的核心是使得数值比较稳定

## 问题2:批量归一化是不是和权重衰减的作用类似？

答：不会有太多影响

## 问题4: batch norm能用在mlp中吗？

答：可以的，对深度网络效果可能好一些

## 问题6: assert len(X. shape)in(24)这个是什么意思啊

答：python中断言操作

## 问题8:BN是做了线性变換，和加一个线性层有什么区别？

答：加一个线性层比一定学到要的东西，线性层不做归一化操作，数值不能收缩到想要的那个值域去。

## 问题9:不太理解，为啥加了 batch norm收敛时间变短

答：batch norm使得梯度变大一点点，使得可以使用较大一点的学习率，从而学习变快

## 问题10:一般怎么衡量严格对比是否有必要呢？一般什么因素可以在做对比实验的时候稍微忽略？ batch size?学习率？框架？ epoch数？

答：参数太多，开发很贵；通常一个参数调调；四个都挺重要。

## 问题11:我看 pytorch里还有一个是 layernorm,请问和BN的异同

答：不同的，BN是在feature维度对样本做归一化，layernorm样本里边做归一化。有很多相关norm

## 问题12: batch norm可以加在激活函数之后么？

答：一般不用在之后，可以试一下

